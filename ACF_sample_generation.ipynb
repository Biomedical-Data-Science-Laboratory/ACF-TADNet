{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, re\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import multiply\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_chr_from_base(base: str) -> str:\n",
    "    # ex) GM12878_chr10_25kb -> chr10\n",
    "    m = re.search(r'_(chr[^_]+)_', base)\n",
    "    if not m:\n",
    "        raise ValueError(f\"cannot parse chromosome from: {base}\")\n",
    "    return m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submatrices\n",
    "\n",
    "def get_ontad_all_bins(tad_file: str) -> np.ndarray:\n",
    "    df = pd.read_csv(tad_file, header=None, sep=r'\\s+', comment='#')\n",
    "    if len(df) == 0:\n",
    "        return np.array([], dtype=int)\n",
    "    df = df.iloc[1:, :]  # skip level 0\n",
    "    starts = df.iloc[:, 0].astype(int).to_numpy() - 1\n",
    "    ends   = df.iloc[:, 1].astype(int).to_numpy() - 1\n",
    "    return np.unique(np.concatenate([starts, ends]))\n",
    "\n",
    "def load_hic_bins_from_bed(boundary_bed: str, chrom: str, res: int = 25000, use_center: bool = True) -> np.ndarray:\n",
    "    if not os.path.exists(boundary_bed):\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    df = pd.read_csv(boundary_bed, sep='\\t', header=None, comment='#')\n",
    "    if df.shape[1] < 3:\n",
    "        raise ValueError(f\"unexpected columns in {boundary_bed}\")\n",
    "    df = df[df.iloc[:, 0] == chrom].copy()\n",
    "    if df.empty:\n",
    "        return np.array([], dtype=int)\n",
    "\n",
    "    start = df.iloc[:, 1].astype(np.int64).to_numpy()\n",
    "    end   = df.iloc[:, 2].astype(np.int64).to_numpy()\n",
    "\n",
    "    if use_center:\n",
    "        pos = (start + end) // 2\n",
    "        bins = np.rint(pos / res).astype(int)\n",
    "    else:\n",
    "        bins = (start // res).astype(int)\n",
    "    return np.unique(bins)\n",
    "\n",
    "def _intersect_with_tol(a: np.ndarray, b: np.ndarray, tol: int = 0) -> np.ndarray:\n",
    "    a = np.unique(a); b = np.unique(b)\n",
    "    if tol <= 0:\n",
    "        return np.intersect1d(a, b)\n",
    "    bset = set(b)\n",
    "    hit = []\n",
    "    for x in a:\n",
    "        ok = False\n",
    "        for t in range(-tol, tol+1):\n",
    "            if (x + t) in bset:\n",
    "                ok = True; break\n",
    "        if ok:\n",
    "            hit.append(x)\n",
    "    return np.array(sorted(set(hit)), dtype=int)\n",
    "\n",
    "\n",
    "def generate_samples(matrix, boundary_bins, output_path):\n",
    "    padded_matrix = np.pad(matrix, ((7,7), (7,7)), mode='constant')\n",
    "    samples = []\n",
    "    for bin_idx in range(7, 7 + len(matrix)):\n",
    "        matrix_patch = padded_matrix[bin_idx-7:bin_idx+8, bin_idx-7:bin_idx+8]\n",
    "        label = 1 if (bin_idx - 7) in boundary_bins else 0\n",
    "        samples.append(np.append(matrix_patch.flatten(), label))\n",
    "    samples = np.array(samples).astype('float32')\n",
    "    np.save(output_path, samples)\n",
    "\n",
    "def create_samples_intersection(\n",
    "    matrix_dir: str,\n",
    "    tad_dir: str,\n",
    "    hic_out_dir: str,          # hicFindTADs --outPrefix directory (where *_boundaries.bed is)\n",
    "    output_dir: str,\n",
    "    tol: int = 0\n",
    "):\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in sorted(os.listdir(matrix_dir)):\n",
    "\n",
    "        base = os.path.splitext(file_name)[0]  # ex) GM12878_chr10_25kb\n",
    "        chrom = _parse_chr_from_base(base)\n",
    "\n",
    "        matrix_path = os.path.join(matrix_dir, file_name)\n",
    "        tad_path    = os.path.join(tad_dir, base + '.tad')\n",
    "        hic_bed     = os.path.join(hic_out_dir, base + '_boundaries.bed')\n",
    "\n",
    "        if not os.path.exists(matrix_path):\n",
    "            print(f\"[WARN] missing matrix: {matrix_path}\"); continue\n",
    "        if not os.path.exists(tad_path):\n",
    "            print(f\"[WARN] missing tad: {tad_path}\"); continue\n",
    "        if not os.path.exists(hic_bed):\n",
    "            print(f\"[WARN] missing hic boundaries: {hic_bed}\"); continue\n",
    "\n",
    "        matrix = np.loadtxt(matrix_path)\n",
    "\n",
    "        # OnTAD (excluding level 0)\n",
    "        ontad_bins_all = get_ontad_all_bins(tad_path)\n",
    "\n",
    "        # HiCExplorer: boundaries.bed → 25kb bin\n",
    "        hic_bins = load_hic_bins_from_bed(hic_bed, chrom=chrom, res=25000, use_center=True)\n",
    "\n",
    "        # Intersection\n",
    "        inter_bins = _intersect_with_tol(ontad_bins_all, hic_bins, tol=tol)\n",
    "        pos_set = set(inter_bins.tolist())\n",
    "\n",
    "        output_path = os.path.join(output_dir, file_name.replace('.txt', '.npy'))\n",
    "\n",
    "        generate_samples(matrix, pos_set, output_path)\n",
    "        print(f\"[OK] {base}: |OnTAD(all)|={len(ontad_bins_all)} |HiC|={len(hic_bins)} |∩|={len(pos_set)} -> {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "create_samples_intersection(\n",
    "    matrix_dir = '/hic/matrix/txt/file/path',\n",
    "    tad_dir    = '/ontad/output//tad/file/path',\n",
    "    hic_out_dir= '/HiCexplorer/output/bed/file/path',\n",
    "    output_dir = '/training/dataset/output/directory',\n",
    "    tol        = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "create_samples_intersection(\n",
    "    matrix_dir = '/hic/matrix/txt/file/path',\n",
    "    tad_dir    = '/ontad/output//tad/file/path',\n",
    "    hic_out_dir= '/HiCexplorer/output/bed/file/path',\n",
    "    output_dir = '/test/dataset/output/directory',\n",
    "    tol        = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c503222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf9efc0",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deb62c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only training data\n",
    "def training_postprocess(submatrix_data, output_path):\n",
    "    pos_samples = submatrix_data[submatrix_data[:, -1] == 1]\n",
    "    neg_samples = submatrix_data[submatrix_data[:, -1] == 0]\n",
    "\n",
    "    rotated_samples = []\n",
    "    for sample in pos_samples:\n",
    "        matrix = sample[:-1].reshape(15, 15)\n",
    "        rotated = np.rot90(matrix, k=-1).flatten()\n",
    "        rotated_sample = np.append(rotated, 1)\n",
    "        rotated_samples.append(rotated_sample)\n",
    "    rotated_samples = np.array(rotated_samples)\n",
    "\n",
    "    num_pos = len(pos_samples) + len(rotated_samples)\n",
    "    neg_limit = min(len(neg_samples), num_pos * 4)\n",
    "    sampled_neg = neg_samples[np.random.choice(len(neg_samples), size=neg_limit, replace=False)]\n",
    "\n",
    "    final_data = np.vstack([pos_samples, rotated_samples, sampled_neg])\n",
    "    np.random.shuffle(final_data)\n",
    "    np.save(output_path, final_data)\n",
    "\n",
    "    print(f\"Processed and saved to {output_path}\")\n",
    "    print(f\"Positive samples (including augmented): {len(pos_samples) + len(rotated_samples)}\")\n",
    "    print(f\"Negative samples (limited): {len(sampled_neg)}\")\n",
    "\n",
    "def process_entire_dataset(training_root_folder):\n",
    "    print(f\"\\nProcessing (flat): {training_root_folder} ...\")\n",
    "\n",
    "    for file_name in sorted(os.listdir(training_root_folder)):\n",
    "        if not file_name.endswith('.npy'):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(training_root_folder, file_name)\n",
    "        output_file = file_name.replace('.npy', '_processed.npy')\n",
    "        output_path = os.path.join(training_root_folder, output_file)\n",
    "\n",
    "        submatrix_data = np.load(file_path)\n",
    "        training_postprocess(submatrix_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c264b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_root = 'postprocessed/training/dataset/output/directory'\n",
    "process_entire_dataset(training_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256b4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d7447e",
   "metadata": {},
   "source": [
    "## Generate model weight file(h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cca1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel = input_feature.shape[-1]\n",
    "    filters = max(1, int(channel//ratio))\n",
    "    shared_layer_one = tf.keras.layers.Dense(filters,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = tf.keras.layers.Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = tf.keras.layers.Reshape((1,1,channel))(avg_pool)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = tf.keras.layers.Reshape((1,1,channel))(max_pool)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "   \n",
    "\n",
    "    cbam_feature = tf.keras.layers.Add()([avg_pool,max_pool])\n",
    "    cbam_feature = tf.keras.layers.Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf22a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACF(layers.Layer):\n",
    "    def __init__(self, D=1, mid_ch=32, k_row=5, k_col=5, return_center=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.D = D\n",
    "        self.mid_ch = mid_ch\n",
    "        self.k_row = k_row\n",
    "        self.k_col = k_col\n",
    "        self.return_center = return_center\n",
    "\n",
    "        self.row_conv = keras.Sequential([\n",
    "            layers.SeparableConv2D(self.mid_ch, kernel_size=(3, self.k_row),\n",
    "                                   padding='same', activation='relu'),\n",
    "            layers.Conv2D(self.mid_ch, kernel_size=1, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.col_conv = keras.Sequential([\n",
    "            layers.SeparableConv2D(self.mid_ch, kernel_size=(self.k_col, 3),\n",
    "                                   padding='same', activation='relu'),\n",
    "            layers.Conv2D(self.mid_ch, kernel_size=1, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.gate_row = keras.Sequential([layers.Conv2D(1, 1, activation='sigmoid')])\n",
    "        self.gate_col = keras.Sequential([layers.Conv2D(1, 1, activation='sigmoid')])\n",
    "\n",
    "        self.head = keras.Sequential([\n",
    "            layers.Conv2D(self.mid_ch, 1, activation='relu'),\n",
    "            layers.Conv2D(1, 1, activation=None)\n",
    "        ])\n",
    "\n",
    "    def _shift(self, x, dy=0, dx=0):\n",
    "        B, H, W, C = tf.unstack(tf.shape(x))\n",
    "        pad_top  = tf.maximum( dy, 0)\n",
    "        pad_bot  = tf.maximum(-dy, 0)\n",
    "        pad_left = tf.maximum( dx, 0)\n",
    "        pad_right= tf.maximum(-dx, 0)\n",
    "        xpad = tf.pad(x, [[0,0],[pad_top,pad_bot],[pad_left,pad_right],[0,0]])\n",
    "        y0 = tf.maximum(-dy, 0)\n",
    "        y1 = y0 + H\n",
    "        x0 = tf.maximum(-dx, 0)\n",
    "        x1 = x0 + W\n",
    "        return xpad[:, y0:y1, x0:x1, :]\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        diffs_row = []\n",
    "        diffs_col = []\n",
    "        for d in range(1, self.D+1):\n",
    "            down = self._shift(x, dy=+d)\n",
    "            up   = self._shift(x, dy=-d)\n",
    "            right= self._shift(x, dx=+d)\n",
    "            left = self._shift(x, dx=-d)\n",
    "\n",
    "            dr = down - x\n",
    "            ur = up   - x\n",
    "            rc = right- x\n",
    "            lc = left - x\n",
    "\n",
    "            diffs_row += [dr, ur, tf.abs(dr), tf.abs(ur)]\n",
    "            diffs_col += [rc, lc, tf.abs(rc), tf.abs(lc)]\n",
    "\n",
    "        Fr = tf.concat(diffs_row, axis=-1)\n",
    "        Fc = tf.concat(diffs_col, axis=-1)\n",
    "\n",
    "        R = self.row_conv(Fr)\n",
    "        C = self.col_conv(Fc)\n",
    "\n",
    "        gr = self.gate_row(R)\n",
    "        gc = self.gate_col(C)\n",
    "\n",
    "        R = R * gr\n",
    "        C = C * gc\n",
    "\n",
    "        fused = tf.concat([R, C], axis=-1)\n",
    "        P = self.head(fused)\n",
    "\n",
    "        if self.return_center:\n",
    "            H = tf.shape(P)[1]\n",
    "            W = tf.shape(P)[2]\n",
    "            i = H // 2\n",
    "            j = W // 2\n",
    "            center = P[:, i:i+1, j:j+1, :]\n",
    "            center = tf.reshape(center, [tf.shape(P)[0], 1])\n",
    "            return center\n",
    "        else:\n",
    "            return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6195b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_channel_only(x, ratio=8):\n",
    "    return channel_attention(x, ratio)\n",
    "\n",
    "def init_model():\n",
    "    inputs = layers.Input(shape=(15,15,1))\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.MaxPooling2D((3,3), strides=(3,3))(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = cbam_channel_only(x, ratio=8)\n",
    "\n",
    "    acf_logit = ACF(D=2, mid_ch=32, k_row=5, k_col=5, return_center=True)(x)\n",
    "\n",
    "    x = layers.Dense(128, activation=None, kernel_initializer='he_normal')(acf_logit)\n",
    "    x = layers.PReLU()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation=None, kernel_initializer='he_normal')(x)\n",
    "    x = layers.PReLU()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "        metrics=[keras.metrics.TruePositives(name='tp'),\n",
    "                 keras.metrics.FalsePositives(name='fp'),\n",
    "                 keras.metrics.TrueNegatives(name='tn'),\n",
    "                 keras.metrics.FalseNegatives(name='fn'),\n",
    "                 keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                 keras.metrics.Precision(name='precision'),\n",
    "                 keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b493ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a):\n",
    "    oshape = a.shape\n",
    "    a = a.reshape(-1,15).astype('float32')\n",
    "    mean = np.mean(a,axis = 1).reshape(-1,1)\n",
    "    a = a - mean\n",
    "    sqrt = (np.sqrt(a.var(axis =1))+1e-10).reshape(-1,1)\n",
    "    a = a/sqrt\n",
    "    return a.reshape(oshape)\n",
    "    \n",
    "class load_testdata(Sequence):\n",
    "    def __init__(self, x_y_set, batch_size):\n",
    "        self.x_y_set = x_y_set\n",
    "        self.x = self.x_y_set[:,:]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return math.floor(len(self.x) / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_x = batch_x.reshape(-1,15,15,1)\n",
    "           \n",
    "        return np.array(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_processed_npy(folder_path):\n",
    "    all_data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('_processed.npy'):\n",
    "            path = os.path.join(folder_path, file)\n",
    "            data = np.load(path)\n",
    "            all_data.append(data)\n",
    "    if all_data:\n",
    "        return np.vstack(all_data)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38611340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(train_data, output_h5):\n",
    "    X_flat = train_data[:, :-1].astype('float32')\n",
    "    X_flat = fun(X_flat) \n",
    "    X = X_flat.reshape(-1, 15, 15, 1) \n",
    "    y = train_data[:, -1].astype('float32')\n",
    "    model = init_model()\n",
    "    model.fit(X, y, epochs=30, batch_size=128, verbose=2)\n",
    "    model.save_weights(output_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44354f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/training/dataset/directory\"\n",
    "output_dir = \"/weight/output/directory\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "level_path = base_dir\n",
    "data = load_all_processed_npy(level_path)\n",
    "if data is not None:\n",
    "    h5_path = os.path.join(output_dir, \"model.weights.h5\")\n",
    "    train_and_save_model(data, h5_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepTAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
