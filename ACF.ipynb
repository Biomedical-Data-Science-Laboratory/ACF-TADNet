{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b091df4c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a95666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import math\n",
    "\n",
    "from scipy.stats import ranksums\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import multiply\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel = input_feature.shape[-1]\n",
    "    filters = max(1, int(channel//ratio))\n",
    "    shared_layer_one = tf.keras.layers.Dense(filters,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = tf.keras.layers.Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = tf.keras.layers.GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = tf.keras.layers.Reshape((1,1,channel))(avg_pool)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "\n",
    "    max_pool = tf.keras.layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = tf.keras.layers.Reshape((1,1,channel))(max_pool)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "   \n",
    "\n",
    "    cbam_feature = tf.keras.layers.Add()([avg_pool,max_pool])\n",
    "    cbam_feature = tf.keras.layers.Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c75d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACF(layers.Layer):\n",
    "\n",
    "    def __init__(self, D=1, mid_ch=32, k_row=5, k_col=5, return_center=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.D = D\n",
    "        self.mid_ch = mid_ch\n",
    "        self.k_row = k_row\n",
    "        self.k_col = k_col\n",
    "        self.return_center = return_center\n",
    "\n",
    "        self.row_conv = keras.Sequential([\n",
    "            layers.SeparableConv2D(self.mid_ch, kernel_size=(3, self.k_row),\n",
    "                                   padding='same', activation='relu'),\n",
    "            layers.Conv2D(self.mid_ch, kernel_size=1, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.col_conv = keras.Sequential([\n",
    "            layers.SeparableConv2D(self.mid_ch, kernel_size=(self.k_col, 3),\n",
    "                                   padding='same', activation='relu'),\n",
    "            layers.Conv2D(self.mid_ch, kernel_size=1, activation='relu')\n",
    "        ])\n",
    "\n",
    "        self.gate_row = keras.Sequential([layers.Conv2D(1, 1, activation='sigmoid')])\n",
    "        self.gate_col = keras.Sequential([layers.Conv2D(1, 1, activation='sigmoid')])\n",
    "\n",
    "        self.head = keras.Sequential([\n",
    "            layers.Conv2D(self.mid_ch, 1, activation='relu'),\n",
    "            layers.Conv2D(1, 1, activation=None)\n",
    "        ])\n",
    "\n",
    "    def _shift(self, x, dy=0, dx=0):\n",
    "        B, H, W, C = tf.unstack(tf.shape(x))\n",
    "        pad_top  = tf.maximum( dy, 0)\n",
    "        pad_bot  = tf.maximum(-dy, 0)\n",
    "        pad_left = tf.maximum( dx, 0)\n",
    "        pad_right= tf.maximum(-dx, 0)\n",
    "        xpad = tf.pad(x, [[0,0],[pad_top,pad_bot],[pad_left,pad_right],[0,0]])\n",
    "        y0 = tf.maximum(-dy, 0)\n",
    "        y1 = y0 + H\n",
    "        x0 = tf.maximum(-dx, 0)\n",
    "        x1 = x0 + W\n",
    "        return xpad[:, y0:y1, x0:x1, :]\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        diffs_row = []\n",
    "        diffs_col = []\n",
    "        for d in range(1, self.D+1):\n",
    "            down = self._shift(x, dy=+d)\n",
    "            up   = self._shift(x, dy=-d)\n",
    "            right= self._shift(x, dx=+d)\n",
    "            left = self._shift(x, dx=-d)\n",
    "\n",
    "            dr = down - x\n",
    "            ur = up   - x\n",
    "            rc = right- x\n",
    "            lc = left - x\n",
    "\n",
    "            diffs_row += [dr, ur, tf.abs(dr), tf.abs(ur)]\n",
    "            diffs_col += [rc, lc, tf.abs(rc), tf.abs(lc)]\n",
    "\n",
    "        Fr = tf.concat(diffs_row, axis=-1)\n",
    "        Fc = tf.concat(diffs_col, axis=-1)\n",
    "\n",
    "        R = self.row_conv(Fr)\n",
    "        C = self.col_conv(Fc)\n",
    "\n",
    "        gr = self.gate_row(R)\n",
    "        gc = self.gate_col(C)\n",
    "\n",
    "        R = R * gr\n",
    "        C = C * gc\n",
    "\n",
    "        fused = tf.concat([R, C], axis=-1)\n",
    "        P = self.head(fused)\n",
    "\n",
    "        if self.return_center:\n",
    "            H = tf.shape(P)[1]\n",
    "            W = tf.shape(P)[2]\n",
    "            i = H // 2\n",
    "            j = W // 2\n",
    "            center = P[:, i:i+1, j:j+1, :]\n",
    "            center = tf.reshape(center, [tf.shape(P)[0], 1])\n",
    "            return center\n",
    "        else:\n",
    "            return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05939927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam_channel_only(x, ratio=8):\n",
    "    return channel_attention(x, ratio)\n",
    "\n",
    "def init_model():\n",
    "    inputs = layers.Input(shape=(15,15,1))\n",
    "\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu', kernel_initializer='he_normal')(inputs)\n",
    "    x = layers.MaxPooling2D((3,3), strides=(3,3))(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu', kernel_initializer='he_normal')(x)\n",
    "    x = cbam_channel_only(x, ratio=8)\n",
    "\n",
    "    acf_logit = ACF(D=2, mid_ch=32, k_row=5, k_col=5, return_center=True)(x) \n",
    "\n",
    "    x = layers.Dense(128, activation=None, kernel_initializer='he_normal')(acf_logit)\n",
    "    x = layers.PReLU()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation=None, kernel_initializer='he_normal')(x)\n",
    "    x = layers.PReLU()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "        metrics=[keras.metrics.TruePositives(name='tp'),\n",
    "                 keras.metrics.FalsePositives(name='fp'),\n",
    "                 keras.metrics.TrueNegatives(name='tn'),\n",
    "                 keras.metrics.FalseNegatives(name='fn'),\n",
    "                 keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "                 keras.metrics.Precision(name='precision'),\n",
    "                 keras.metrics.Recall(name='recall')]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = init_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(a):\n",
    "    oshape = a.shape\n",
    "    a = a.reshape(-1,15).astype('float32')\n",
    "    mean = np.mean(a,axis = 1).reshape(-1,1)\n",
    "    a = a - mean\n",
    "    sqrt = (np.sqrt(a.var(axis =1))+1e-10).reshape(-1,1)\n",
    "    a = a/sqrt\n",
    "    return a.reshape(oshape)\n",
    "    \n",
    "class load_testdata(Sequence):\n",
    "    def __init__(self, x_y_set, batch_size):\n",
    "        self.x_y_set = x_y_set\n",
    "        self.x = self.x_y_set[:,:]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return math.floor(len(self.x) / self.batch_size)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        batch_x = batch_x.reshape(-1,15,15,1)\n",
    "           \n",
    "        return np.array(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd85326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "262e59b1",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc56dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir  = '/test/dataset/directory'\n",
    "model_path= 'path/to/model.weights.h5'\n",
    "save_dir  = 'path/to/predict_result'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model = init_model()\n",
    "model.load_weights(model_path)\n",
    "\n",
    "for fname in sorted(os.listdir(test_dir)):\n",
    "    if not fname.endswith('.npy'):\n",
    "        continue\n",
    "    fpath = os.path.join(test_dir, fname)\n",
    "    pre_data = np.load(fpath)\n",
    "\n",
    "    random.seed(0)\n",
    "    np.random.shuffle(pre_data)\n",
    "\n",
    "    data1 = fun(pre_data[:, :-1])\n",
    "    data  = pre_data[:, :-1]\n",
    "\n",
    "    batch_size = 32\n",
    "    preprocessed_data = load_testdata(data1, batch_size)\n",
    "    epo = math.floor(len(pre_data) / batch_size)\n",
    "\n",
    "    predictions = model.predict(preprocessed_data, batch_size=batch_size, verbose=0)\n",
    "    predictions = np.array(predictions.flatten()).reshape(-1, 1)\n",
    "\n",
    "    merged_data = np.concatenate((data[:batch_size * epo], predictions), axis=1)\n",
    "    merged_data[:, -1] = (merged_data[:, -1] >= 0.5).astype(int)\n",
    "\n",
    "    save_path = os.path.join(save_dir, fname.replace('.npy', '_pred.npy'))\n",
    "    np.save(save_path, merged_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_submatrix(big_matrix, small_matrix):\n",
    "    big_rows, big_cols = big_matrix.shape\n",
    "    small_rows, small_cols = small_matrix.shape\n",
    "    for i in range(big_rows - small_rows + 1):\n",
    "        if np.allclose(big_matrix[i:i + small_rows, i:i + small_cols], small_matrix):\n",
    "            return i + 7, i + 8\n",
    "    return None\n",
    "\n",
    "pred_dir = '/path/to/predict_result'\n",
    "save_dir = '/path/to/predict_results_boundary'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for fname in sorted(os.listdir(pred_dir)):\n",
    "    if not fname.endswith('_pred.npy'):\n",
    "        continue\n",
    "\n",
    "    pred_data = np.load(os.path.join(pred_dir, fname))\n",
    "    positive_data = pred_data[pred_data[:, -1] == 1, :225]\n",
    "\n",
    "    base_name = fname.replace('_pred.npy', '.txt')\n",
    "    matrix_path = os.path.join('/path/to/test/hic/matrix/txt/file', base_name)\n",
    "    big_matrix = np.loadtxt(matrix_path)\n",
    "\n",
    "    results = []\n",
    "    for i in range(positive_data.shape[0]):\n",
    "        small_matrix = positive_data[i].reshape((15, 15))\n",
    "        result = find_submatrix(big_matrix, small_matrix)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "\n",
    "    sorted_results = sorted(results)\n",
    "    with open(os.path.join(save_dir, fname.replace('_pred.npy', '_bound.txt')), 'w') as f:\n",
    "        for r in sorted_results:\n",
    "            f.write(f\"{r[0]}\\t{r[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count TAD boundary bins\n",
    "from pathlib import Path\n",
    "\n",
    "def count_pos_lastcol(dir_path):\n",
    "    dirp = Path(dir_path)\n",
    "    total = 0\n",
    "    for f in sorted(dirp.glob(\"*.npy\")):\n",
    "        arr = np.load(f)\n",
    "        cnt = int(np.sum(arr[:, -1] == 1))\n",
    "        print(f\"{f.name}: {cnt}\")\n",
    "        total += cnt\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"TOTAL: {total}\")\n",
    "\n",
    "count_pos_lastcol(\"/path/to/predict_result\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540df97e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54c72005",
   "metadata": {},
   "source": [
    "## false positive filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67702386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(matrix_path):\n",
    "    return np.loadtxt(matrix_path)\n",
    "\n",
    "def parse_prediction(pred_path):\n",
    "    data = np.loadtxt(pred_path, dtype=int)\n",
    "    return data[:, 0] if data.ndim == 2 else np.array([data[0]])\n",
    "\n",
    "def find_gap_regions(matrix, window_size):\n",
    "    n = matrix.shape[0]\n",
    "    gap = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        s = max(0, i - window_size)\n",
    "        e = min(n, i + window_size + 1)\n",
    "        if np.sum(matrix[i, s:e]) == 0:\n",
    "            gap[i] = -0.5\n",
    "    return np.where(gap == -0.5)[0]\n",
    "\n",
    "def which_process_regions(rmv_idx, n_bins, min_size=3):\n",
    "    proc = sorted(set(range(n_bins)) - set(rmv_idx))\n",
    "    if not proc:\n",
    "        return []\n",
    "\n",
    "    regions = []\n",
    "    start = proc[0]\n",
    "    for i in range(1, len(proc)):\n",
    "        if proc[i] - proc[i - 1] > 1:\n",
    "            if proc[i - 1] - start + 1 >= min_size:\n",
    "                regions.append((start, proc[i - 1]))\n",
    "            start = proc[i]\n",
    "    if proc[-1] - start + 1 >= min_size:\n",
    "        regions.append((start, proc[-1]))\n",
    "    return regions\n",
    "\n",
    "def get_upstream_triangle(matrix, i, size):\n",
    "    low = max(0, i - size)\n",
    "    tri = matrix[low:i+1, low:i+1]\n",
    "    return tri[np.triu_indices(tri.shape[0], k=1)]\n",
    "\n",
    "def get_downstream_triangle(matrix, i, size):\n",
    "    n = matrix.shape[0]\n",
    "    if i >= n - 1:\n",
    "        return np.array([])\n",
    "    up = min(n, i + size + 1)\n",
    "    tri = matrix[i+1:up, i+1:up]\n",
    "    return tri[np.triu_indices(tri.shape[0], k=1)]\n",
    "\n",
    "def get_diamond_matrix(matrix, i, size):\n",
    "    n = matrix.shape[0]\n",
    "    new = np.full((size, size), np.nan)\n",
    "    for k in range(size):\n",
    "        row_idx = i - k\n",
    "        if row_idx < 0 or i >= n:\n",
    "            continue\n",
    "        l = min(i + 1, n)\n",
    "        u = min(i + size + 1, n)\n",
    "        new[size - k - 1, :u - l] = matrix[row_idx, l:u]\n",
    "    return new.flatten()\n",
    "\n",
    "def compute_pvalues(matrix, region, size):\n",
    "    pvalues = np.ones(matrix.shape[0])\n",
    "    for i in range(region[0], region[1] + 1):\n",
    "        diamond = get_diamond_matrix(matrix, i, size)\n",
    "        upstream = get_upstream_triangle(matrix, i, size)\n",
    "        downstream = get_downstream_triangle(matrix, i, size)\n",
    "        compare = np.concatenate([upstream, downstream])\n",
    "        if len(diamond) == 0 or len(compare) == 0:\n",
    "            continue\n",
    "        _, p = ranksums(diamond[~np.isnan(diamond)], compare)\n",
    "        pvalues[i] = p\n",
    "    return pvalues\n",
    "\n",
    "def filter_predictions(matrix_path, pred_path, output_path, window_size=8):\n",
    "    matrix = load_matrix(matrix_path)\n",
    "    pred_idx = parse_prediction(pred_path)\n",
    "    n_bins = matrix.shape[0]\n",
    "\n",
    "    local_ext = np.full(n_bins, 0.0)\n",
    "    local_ext[pred_idx] = -1\n",
    "\n",
    "    gap_idx = find_gap_regions(matrix, window_size)\n",
    "    proc_regions = which_process_regions(gap_idx, n_bins)\n",
    "\n",
    "    scaled = matrix.copy()\n",
    "    for d in range(1, 2 * window_size + 1):\n",
    "        row_idx = np.arange(matrix.shape[0] - d)\n",
    "        col_idx = row_idx + d\n",
    "        values = scaled[row_idx, col_idx]\n",
    "        mean = np.mean(values)\n",
    "        std = np.std(values) + 1e-10\n",
    "        scaled[row_idx, col_idx] = (values - mean) / std\n",
    "\n",
    "    pvalues = np.ones(n_bins)\n",
    "    for region in proc_regions:\n",
    "        p = compute_pvalues(scaled, region, window_size)\n",
    "        pvalues[region[0]:region[1]+1] = p[region[0]:region[1]+1]\n",
    "\n",
    "    final_ext = np.copy(local_ext)\n",
    "    for i in range(n_bins):\n",
    "        if local_ext[i] == -1 and pvalues[i] < 0.05:\n",
    "            final_ext[i] = -1\n",
    "        elif local_ext[i] == -1:\n",
    "            final_ext[i] = 0\n",
    "\n",
    "    now_local = np.where(final_ext == -1)[0]\n",
    "    # 인접한 boundary가 있다면 하나만 남김\n",
    "    if len(now_local) >= 2:\n",
    "        filtered = []\n",
    "        skip = False\n",
    "        for i in range(len(now_local)-1):\n",
    "            if skip:\n",
    "                skip = False\n",
    "                continue\n",
    "            if now_local[i] + 1 == now_local[i+1]:\n",
    "                keep = now_local[i] if pvalues[now_local[i]] < pvalues[now_local[i+1]] else now_local[i+1]\n",
    "                filtered.append(keep)\n",
    "                skip = True\n",
    "            else:\n",
    "                filtered.append(now_local[i])\n",
    "        if not skip:\n",
    "            filtered.append(now_local[-1])\n",
    "        now_local = sorted(set(filtered))\n",
    "\n",
    "    results = [(x, x+1) for x in now_local]\n",
    "    with open(output_path, 'w') as f:\n",
    "        for a, b in results:\n",
    "            f.write(f\"{a}\\t{b}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 전체 파일 반복 수행 (flat) ===\n",
    "pred_dir = \"/path/to/predict_results_boundary\"\n",
    "save_dir = \"/path/to/final_filtered\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for fname in sorted(os.listdir(pred_dir)):\n",
    "    if not fname.endswith(\"_bound.txt\"):\n",
    "        continue\n",
    "    matrix_fname = fname.replace(\"_bound.txt\", \".txt\")\n",
    "    matrix_path = os.path.join(\"/path/to/test/hic/matrix/txt/file\", matrix_fname)\n",
    "    pred_path = os.path.join(pred_dir, fname)\n",
    "    output_path = os.path.join(save_dir, fname.replace(\"_bound.txt\", \"_filtered.txt\"))\n",
    "\n",
    "    print(f\"[flat] Filtering {fname}\")\n",
    "    filter_predictions(matrix_path, pred_path, output_path, window_size=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepTAD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
